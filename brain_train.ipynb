{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darkchess Brain AI Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: Random (UNKNOWN)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "========================================\n",
      "Action: (14, 14)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: BetterEvalAlphaBeta-6 (BLACK)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "========================================\n",
      "Action: (20, 20)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: Random (RED)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "========================================\n",
      "Action: (8, 8)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: BetterEvalAlphaBeta-6 (BLACK)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "========================================\n",
      "Action: (16, 16)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: Random (RED)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "========================================\n",
      "Action: (31, 31)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: BetterEvalAlphaBeta-6 (BLACK)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: (9, 9)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: Random (RED)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 車 | 圞 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: (10, 10)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: BetterEvalAlphaBeta-6 (BLACK)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 車 | 俥 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: (9, 10)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: Random (RED)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 囗 | 車 | 圞 | 圞 | 圞 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: (13, 13)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: BetterEvalAlphaBeta-6 (BLACK)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 囗 | 車 | 圞 | 圞 | 傌 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 圞 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: (22, 22)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: Random (RED)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 囗 | 車 | 圞 | 圞 | 傌 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 馬 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: (29, 29)\n",
      "\n",
      "========================================\n",
      "Drawstep: 0\n",
      "Turn: BetterEvalAlphaBeta-6 (BLACK)\n",
      "========================================\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 | 圞 |\n",
      "| 卒 | 囗 | 車 | 圞 | 圞 | 傌 | 兵 | 圞 |\n",
      "| 象 | 圞 | 圞 | 圞 | 傌 | 圞 | 馬 | 圞 |\n",
      "| 圞 | 圞 | 圞 | 圞 | 圞 | 卒 | 圞 | 兵 |\n",
      "========================================\n",
      "Action: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m battle \u001b[38;5;241m=\u001b[39m Battle(\n\u001b[1;32m     13\u001b[0m     player1\u001b[38;5;241m=\u001b[39mRandom(),\n\u001b[1;32m     14\u001b[0m     player2\u001b[38;5;241m=\u001b[39mBetterEvalAlphaBeta(\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m     15\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Start the battle\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mbattle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/arena.py:125\u001b[0m, in \u001b[0;36mBattle.play_games\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_record\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# check if the game ends, otherwise update the board and continue\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/arena.py:68\u001b[0m, in \u001b[0;36mBattle.board_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m current_player \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayers[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn]\n\u001b[0;32m---> 68\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_player\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# check if the current player has no action, then the other player wins and the game ends\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/arena.py:139\u001b[0m, in \u001b[0;36mPlayer.action\u001b[0;34m(self, board)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maction\u001b[39m(\u001b[38;5;28mself\u001b[39m, board: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/base.py:22\u001b[0m, in \u001b[0;36mBaseAgent.action\u001b[0;34m(self, board, color, eaten)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_availablesteps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:204\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta._action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_action\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_availablesteps)\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:224\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    222\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    223\u001b[0m new_board[move[\u001b[38;5;241m1\u001b[39m]], new_board[move[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m new_board[move[\u001b[38;5;241m0\u001b[39m]], CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 224\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m val \u001b[38;5;241m>\u001b[39m best_val:\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:249\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    247\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    248\u001b[0m new_board[move[\u001b[38;5;241m1\u001b[39m]], new_board[move[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m new_board[move[\u001b[38;5;241m0\u001b[39m]], CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 249\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(best_val, val)\n\u001b[1;32m    251\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(beta, best_val)\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:224\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    222\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    223\u001b[0m new_board[move[\u001b[38;5;241m1\u001b[39m]], new_board[move[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m new_board[move[\u001b[38;5;241m0\u001b[39m]], CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 224\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m val \u001b[38;5;241m>\u001b[39m best_val:\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:249\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    247\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    248\u001b[0m new_board[move[\u001b[38;5;241m1\u001b[39m]], new_board[move[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m new_board[move[\u001b[38;5;241m0\u001b[39m]], CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 249\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(best_val, val)\n\u001b[1;32m    251\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(beta, best_val)\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:224\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    222\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    223\u001b[0m new_board[move[\u001b[38;5;241m1\u001b[39m]], new_board[move[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m new_board[move[\u001b[38;5;241m0\u001b[39m]], CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 224\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m val \u001b[38;5;241m>\u001b[39m best_val:\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:249\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    247\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    248\u001b[0m new_board[move[\u001b[38;5;241m1\u001b[39m]], new_board[move[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m new_board[move[\u001b[38;5;241m0\u001b[39m]], CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 249\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(best_val, val)\n\u001b[1;32m    251\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(beta, best_val)\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/agent/agent.py:208\u001b[0m, in \u001b[0;36mBetterEvalAlphaBeta.algorithm\u001b[0;34m(self, board, color, depth, turn, value, alpha, beta)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21malgorithm\u001b[39m(\u001b[38;5;28mself\u001b[39m, board: List[\u001b[38;5;28mstr\u001b[39m], color: \u001b[38;5;28mint\u001b[39m, depth: \u001b[38;5;28mint\u001b[39m, turn: \u001b[38;5;28mint\u001b[39m, value: \u001b[38;5;28mint\u001b[39m, alpha: \u001b[38;5;28mint\u001b[39m, beta: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m     moves \u001b[38;5;241m=\u001b[39m \u001b[43mavailable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m moves:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/utils.py:38\u001b[0m, in \u001b[0;36mavailable\u001b[0;34m(board, color)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# check if the chess in index of board is the same color\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m get_chess_index(matrix[i][j]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m color \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m) \\\n\u001b[0;32m---> 38\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;241m7\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_chess_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m13\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m color \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \\\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m matrix[i][j] \u001b[38;5;241m==\u001b[39m CHESS[\u001b[38;5;241m15\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# check and append dark chess (code: *)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/darkchess-robot/brain/utils.py:14\u001b[0m, in \u001b[0;36mget_chess_index\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_chess_index\u001b[39m(code: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, piece \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(CHESS):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpiece\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m:\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m index\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brain.arena import Battle\n",
    "from brain.agent import (\n",
    "    Human,\n",
    "    Random,\n",
    "    MinMax,\n",
    "    AlphaBeta,\n",
    "    BetterEval,\n",
    "    BetterEvalAlphaBeta\n",
    ")\n",
    "\n",
    "# Choose two agents from the list above\n",
    "battle = Battle(\n",
    "    player1=Random(),\n",
    "    player2=BetterEval(6),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Start the battle\n",
    "battle.play_games()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training QL (Q-Table) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_MODE = \"8x4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training QL (Q-Table) with MCTS Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_MODE = \"8x4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Reinforcement Learning (DRL) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_MODE = \"8x4\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
